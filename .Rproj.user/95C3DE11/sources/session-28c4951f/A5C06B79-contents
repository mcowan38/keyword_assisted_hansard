---
title: "SOCC70: Visualization Memo"
author: "Mike Cowan"
format: 
  html:
    embed-resources: true
    df-print: paged
editor: visual
---

## 1. Packages & CSV conversion (optional)

```{r}
library(tidyverse)
library(quanteda)
library(keyATM)
library(readtext)

# Define output folder
# output_folder <- "speeches_text"
# dir.create(output_folder, showWarnings = FALSE)  # Create directory if it doesn't exist

# Save each speech as .txt
# for (i in 1:nrow(data)) {
#   file_name <- paste0(output_folder, "/speech_", i, ".txt")  
#   writeLines(enc2utf8(data$speechtext[i]), file_name, useBytes = TRUE)  # Ensure UTF-8 encoding
# }
```

## 2. Data

```{r}
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)

# Ensure speechdate is in Date format
data$speechdate <- as.Date(data$speechdate)
```

## 3. Preprocessing

```{r}
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")

# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")

# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
  key_corpus,
  remove_punct = TRUE,
  remove_numbers = TRUE,
  remove_symbols = TRUE,
  remove_separators = TRUE,
  remove_url = TRUE
) %>%
  tokens_tolower() %>%
  tokens_remove(
    c(stopwords("english"), "shall", "may", "upon", "without", "canada",
      "canadian", "canadians") # Corpus-specific stopwords
  ) %>%
  tokens_select(min_nchar = 3)

# Remove metadata artifacts (explicit cleaning)
unwanted_terms <- c("data-hocid", "data-originallang", "href", "title", "bills", 
                    "minister", "committee", "politicians", "motion", "order", 
                    "government", "state", "house", "member", "prime")  
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)

# Convert to Document-Feature Matrix (DFM)
dfm_clean <- dfm(tokens_clean) %>%
  dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms

# Check top features to verify cleaning worked
topfeatures(dfm_clean, 20)
```

## 4. Define Keywords

-   The keyATM framework allows you to **seed specific topics with keyword constraints**, meaning that topics will be structured around these predefined terms, guiding the model towards specific areas of interest rather than relying solely on unsupervised topic extraction.

```{r}
# Define keywords for Topic Modeling
keywords <- list(
  antisemitism = c("antisemitism", "racism", "discrimination", "Jew-hatred", "anti-Jewish", "prejudice"),
  israel_palestine = c("israel", "palestine", "zionism", "West Bank", "Gaza", "settlements", "two-state"),
  holocaust = c("holocaust", "nazi", "genocide", "Shoah", "concentration camp", "Holocaust denial"),
  jewish_identity = c("jewish", "judaism", "diaspora", "Jewish heritage", "Hebrew", "kashrut", "synagogue", "Yiddish")
)

```

## 5. Check Keyword Frequency

-   If a keyword appears **too infrequently**, it may not influence the model effectively.
-   Remove or adjust rare keywords before running keyATM.

```{r}
# Convert DFM into keyATM format
keyATM_docs <- keyATM_read(texts = dfm_clean)

# Visualize keyword frequency
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz  # Plot frequency of each keyword

# Save plot
# save_fig(key_viz, "figures/keyword_frequency.pdf", width = 6.5, height = 4)
```

### **a) Observations:**

1.  **Ranking and Proportion Trends:**

    -   Each keyword's ranking is plotted along the **x-axis** (from most to least frequent).

    -   The **y-axis** represents the **proportion of times** each keyword appears in the dataset.

2.  **Topic-Based Keyword Distribution:**

    -   **Topic 1: Antisemitism (red)**

        -   Keywords **"racism," "discrimination," and "antisemitism"** are dominant.

        -   "Racism" appears more frequently than "antisemitism," which suggests antisemitism-related discourse might be framed within a **broader discrimination narrative**.

    -   **Topic 2: Israel-Palestine (green)**

        -   Keywords **"Israel" and "Palestine"** have the highest proportions.

        -   "Zionism" appears with much lower frequency, possibly indicating **less explicit discussion of ideological aspects** in comparison to direct geographic/political mentions.

    -   **Topic 3: Holocaust (blue)**

        -   **"Genocide" and "Holocaust"** are closely ranked, showing that discussions of the Holocaust are linked to **broader themes of genocide**.

        -   "Nazi" appears the least within this topic, indicating that Holocaust discussions may focus on **historical events rather than perpetrators**.

    -   **Topic 4: Jewish Identity (purple)**

        -   **"Community" and "Jewish"** are more frequent than "Judaism," which suggests discourse around Jewish identity may be **framed in communal/social terms rather than religious**.

3.  **Steep Decline in Proportion:**

    -   **Sharp drops in keyword frequency** within each topic suggest that **each topic is dominated by one or two major terms**.

    -   For instance, **“Israel” dominates the Israel-Palestine topic**, whereas **"community" dominates Jewish Identity**.

    -   This implies that certain terms **may disproportionately shape topic modeling outputs**, potentially leading to **skewed topic representation**.

### **b) Suggestions for Refinement:**

1.  **Balance Keyword Frequencies Across Topics:**

    -   Some topics rely heavily on **one dominant keyword** (e.g., "Israel" or "Community").

    -   Consider adding **synonyms** or related phrases to balance keyword weights and **avoid single-word dominance**.

2.  **Explore Context of Lower-Frequency Keywords:**

    -   Keywords like "nazi," "zionism," and "judaism" appear infrequently.

    -   Check whether these terms are **too restrictive** or if they appear in different forms (e.g., "Nazism" instead of "Nazi").

3.  **Test with Adjusted Keyword Lists:**

    -   If the goal is **broader thematic coverage**, consider adjusting keyword groups by adding variations.

    -   e.g., Add "Jewish faith" or "Jewish belief" under **Jewish Identity** if religion is underrepresented.

## 6. keyATM Base (Latent Dirichlet Allocation + Keywords)

-   **Use case:** standard keyword-assisted topic modeling.

-   Good for interpreting latent themes in speeches.

```{r}
model_base <- keyATM(docs = keyATM_docs, keywords = keywords, no_keyword_topics = 3, model = "base")
# Save Model
saveRDS(model_base, file = "./models/keyATM_base.rds")
```

## 7. keyATM Covariate Model (analyzing metadata influence)

-   **Use Case:** If you want to see how different political parties talk about topics.
-   Good for party-level topic variations.

```{r}
model_covariate <- keyATM(docs = keyATM_docs, keywords = keywords, model = "covariates", covariates = data$speaker_party)
# Save Model
saveRDS(model_covariate, file = "models/keyATM_covariate.rds")
```

## 8. keyATM Dynamic Model

-   **Use** case: if you want to explicitly track how topics evolve over time.
-   **Good For:** Historical trend analysis.

```{r}
model_dynamic <- keyATM(docs = keyATM_docs, keywords = keywords, model = "dynamic", time_index = data$speechdate)
# Save Model
saveRDS(model_dynamic, file = "models/keyATM_dynamic.rds")
```

## 9. Model Evaluation & Visualization

### **a) Document-Topic Distribution & Topic-Word Distribution**

```{r}
# Extract document-topic distribution (theta) & topic-word distribution (phi)
doc_topic_dist <- model_base$theta  # Document-topic distribution
word_topic_dist <- model_base$phi   # Topic-word distribution

# Display dimensions of both matrices
dim(doc_topic_dist)  # Rows: Documents, Columns: Topics
dim(word_topic_dist) # Rows: Topics, Columns: Words

# Save distributions for further use
saveRDS(doc_topic_dist, file = "models/document_topic_distribution.rds")
saveRDS(word_topic_dist, file = "models/topic_word_distribution.rds")
```

### b) Top Words Per Topic

```{r}
# Display the top words for each topic
top_words(model_base, n = 10, measure = "probability", show_keyword = TRUE)

# Save top words list
write.csv(top_words(model_base, n = 10), "outputs/top_words_per_topic.csv", row.names = FALSE)
```

### c) Most Representative Documents Per Topic

```{r}
# Identify documents most representative of each topic
top_docs(model_base)

# Save top documents list
write.csv(top_docs(model_base), "outputs/top_documents_per_topic.csv", row.names = FALSE)
```

### **10. Model Fit Diagnostics**

### a) Log-Likelihood and Perplexity

1.  **Likelihood (Left Plot)**

    -   The log-likelihood **steadily increases** and **plateaus after \~1000 iterations**.

    -   This is a **good sign** as it indicates the model is converging towards a stable **optimal point**.

    -   **Early rapid increase (\~0-500 iterations)** suggests initial parameter adjustments, followed by a **slower improvement** as the model refines topic assignments.

2.  **Perplexity (Right Plot)**

    -   **Perplexity decreases sharply at the start** (\~first 500 iterations).

    -   After this initial drop, it continues to decrease **gradually** and stabilizes beyond **1000 iterations**.

    -   **Lower perplexity indicates better model performance**, meaning the model is becoming more confident in assigning words to topics.

```{r}
# Plot log-likelihood and perplexity to evaluate model convergence
fig_modelfit <- plot_modelfit(model_base)
fig_modelfit

# Save figure
save_fig(fig_modelfit, "figures/base_modelfit.pdf", width = 7, height = 5)
```

-   **Log-likelihood increases and stabilizes**, indicating stable topic-word assignments.
-   **Perplexity decreases and levels off**, meaning improved predictive power.
-   The **smooth convergence** of both metrics suggests that **1500 iterations were sufficient**.

### b) Priors for Document-Topic Distribution (α)

```{r}
# Visualizing alpha (prior for document-topic distribution)
fig_alpha <- plot_alpha(model_base)
fig_alpha

# Save figure
save_fig(fig_alpha, "figures/alpha_prior.pdf", width = 7, height = 5)
```

-   **The model is performing well:**

    -   Each topic **adjusts and stabilizes** over time.

    -   The **differences in α values** indicate some topics are more focused while others are more diffuse.

    -   No extreme oscillations or unstable patterns, which suggests a well-calibrated model.

### c) Probability of Keyword Usage in Each Topic (π)

```{r}
# Visualizing π (probability that a topic uses keyword-based word distribution)
fig_pi <- plot_pi(model_base)
fig_pi

# Save figure
save_fig(fig_pi, "figures/pi_distribution.pdf", width = 7, height = 5)
```

1.  **Topic 4 (Jewish Identity) Has the Highest Keyword Influence**

    -   The **highest probability** (\~0.016) occurs in **Topic 4 (Jewish Identity)**.

    -   This suggests that the model relies **heavily on the predefined keywords** for identifying words related to **Jewish identity**.

    -   This could mean that **Jewish identity discourse is more keyword-driven** in the dataset.

2.  **Other Topics Show Lower Reliance on Keywords**

    -   **Topics 1 (Antisemitism), 2 (Israel-Palestine), and 3 (Holocaust)** have much **lower probabilities (\~0.003–0.006)**.

    -   This suggests that for these topics, the model is finding **words beyond the given keywords**, indicating a **broader distribution of words**.

3.  **Potential Over-Reliance on Topic 4 Keywords**

    -   Since **Topic 4's probability is much higher**, this could suggest the keywords used for **Jewish Identity** are **very dominant** in this topic, or the **other topics rely more on contextual inference** rather than strict keyword association.

## 10. Visualize Topic Prevalance

```{r}
# Compute topic proportions
topic_distribution <- colMeans(model_base$theta)

# Convert to DataFrame
topic_df <- data.frame(
  Topic = paste0("Topic_", 1:length(topic_distribution)),
  Proportion = topic_distribution
)

# Plot topic prevalence
ggplot(topic_df, aes(x = Topic, y = Proportion, fill = Topic)) +
  geom_bar(stat = "identity") +
  labs(title = "Prevalence of Topics in KeyATM Model",
       x = "Topic", y = "Average Proportion") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## 11. Time-Series Analysis of Topics

### a) Topic Trends Over Time

```{r}
# Time-series trends of topic proportions
fig_timetrend <- plot_timetrend(
  model_dynamic, time_index_label = data$speechdate, xlab = "Year"
)
fig_timetrend

# Save time-series plot
save_fig(fig_timetrend, "figures/topic_trends_over_time.pdf", width = 7, height = 5)
```

### b) Topic Trends with 90% Credible Intervals

```{r}
# Store theta values to show credible intervals
model_dynamic <- keyATM(
  docs = keyATM_docs,
  no_keyword_topics = 3,
  keywords = keywords,
  model = "dynamic",
  model_settings = list(time_index = data$time_index, num_states = 5),
  options = list(seed = 250, store_theta = TRUE, thinning = 5)
)

# Re-run time-series visualization with credible intervals
fig_timetrend_credible <- plot_timetrend(
  model_dynamic, time_index_label = data$speechdate, xlab = "Year"
)
fig_timetrend_credible

# Save plot
save_fig(fig_timetrend_credible, "figures/topic_trends_with_intervals.pdf", width = 7, height = 5)

```

## 12. Party-wise Topic Differences

### a) Analyzing Topic Proportions by Party

```{r}
# Compute topic proportions by party
strata_topic <- by_strata_DocTopic(
  model_covariate, by_var = "party",
  labels = unique(data$party)
)

# Visualizing topic proportions per party
fig_party_topics <- plot(strata_topic, var_name = "Party")
fig_party_topics

# Save figure
save_fig(fig_party_topics, "figures/partywise_topic_distribution.pdf", width = 7, height = 5)
```

### b) Party-wise Topic Differences with 90% Credible Intervals

```{r}
# Compute topic differences across parties
theta_party_diff <- by_strata_DocTopic(
  model_covariate, by_var = "party",
  labels = unique(data$party)
)

# Visualizing with confidence intervals
fig_party_diff <- plot(theta_party_diff, var_name = "Party", by = "covariate")
fig_party_diff

# Save plot
save_fig(fig_party_diff, "figures/partywise_topic_diff_intervals.pdf", width = 7, height = 5)
```

## 13. Per-Speaker Topic Analysis

```{r}
# Compute speaker-wise topic proportions
speaker_topic_dist <- data %>%
  mutate(speaker = as.factor(speakername)) %>%
  group_by(speaker) %>%
  summarize(across(starts_with("Topic_"), mean, na.rm = TRUE))

# Convert to long format
speaker_long <- speaker_topic_dist %>%
  pivot_longer(cols = starts_with("Topic_"), names_to = "Topic", values_to = "Proportion")

# Plot per-speaker topic proportions
ggplot(speaker_long, aes(x = reorder(speaker, -Proportion), y = Proportion, fill = Topic)) +
  geom_bar(stat = "identity") +
  labs(title = "Speaker-wise Topic Prevalence",
       x = "Speaker", y = "Topic Proportion") +
  coord_flip() +
  theme_minimal()
```
