"Islamic practices"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# View top words per topic
top_words(out)
library(tidyverse)
library(quanteda)
library(keyATM)
library(readtext)
# Define output folder
# output_folder <- "speeches_text"
# dir.create(output_folder, showWarnings = FALSE)  # Create directory if it doesn't exist
# Save each speech as .txt
# for (i in 1:nrow(data)) {
#   file_name <- paste0(output_folder, "/speech_", i, ".txt")
#   writeLines(enc2utf8(data$speechtext[i]), file_name, useBytes = TRUE)  # Ensure UTF-8 encoding
# }
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians", "Mr. Speaker", "can", "one", "also", "bill",
"members", "now") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts (specifically identified in the texts)
unwanted_terms <- c(
# Metadata tags
"data-hocid", "data-originallang", "href", "title", "blockquote",
# Parliamentary jargon
"bills", "minister", "committee", "motion", "order",
# Generic government references
"government", "state", "house", "member", "prime",
# Broad, non-distinct terms
"politicians", "national", "report", "amend", "legislation"
# Context-dependent terms that appear in many discussions
)
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify data cleaning worked
topfeatures(dfm_clean, 20)
set.seed(225)
docs_withSplit <- keyATM_read(texts = dfm_clean, split = 0.3)  # Use 30% for LDA
out <- weightedLDA(
docs              = docs_withSplit$W_split,  # 30% of the corpus
number_of_topics  = 10,  # Define number of topics (k)
model             = "base",
options           = list(seed = 250)
)
# Extract top words from the model (to inform keyword selection)
top_words(out)
# Keywords based on LDA output
keywords <- list(
# **Antisemitism & Islamophobia**
antisemitism = c(
"antisemitism",
"Jew-hatred",
"anti-Jewish",
"neo-nazi",
"white supremacy"
),
islamophobia = c(
"islamophobia",
"anti-Muslim",
"Muslim-hatred",
"Islamophobic",
"religious discrimination"
),
# **Israel vs. Palestine (Geopolitics)**
israel = c(
"israel",
"zionism",
"settlements",
"IDF",
"Netanyahu",
"Jerusalem"
),
palestine = c("palestine", "Gaza", "Hamas", "PLO", "Intifada", "occupation"),
# **Holocaust & Genocide**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"Auschwitz",
"Holocaust denial"
),
# **Jewish & Muslim Identity**
jewish_identity = c("jewish", "judaism", "diaspora", "synagogue"),
muslim_identity = c(
"muslim",
"islam",
"mosque",
"Quran",
"halal",
"sharia",
"Islamic practices"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# View top words per topic
top_words(out)
# Visualize top words per topic
topic_df <- tidy(lda_model, matrix = "beta")
keywords <- list(
# **Antisemitism & Its Framing**
antisemitism = c(
"antisemitism",
"Jew-hatred",
"anti-Jewish",
"prejudice",
"hate",
"discrimination",
"racism",
"stereotypes",
"neo-nazi",
"white supremacy",
"legal",
"hate crime",
"speech laws"
),
# **Zionism & Israel-Palestine Conflict**
israel = c(
"israel",
"zionism",
"settlements",
"IDF",
"Netanyahu",
"Jerusalem",
"Jewish state",
"West Bank"
),
palestine = c(
"palestine",
"Gaza",
"Hamas",
"PLO",
"Intifada",
"occupation",
"Palestinian Authority",
"resistance",
"apartheid"
),
# **The Holocaust & Historical Memory**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"Auschwitz",
"Holocaust denial",
"Final Solution",
"remembrance"
),
# **Framing of Jewish Issues (Law vs. Minority Rights)**
legal_action = c(
"law",
"court",
"police",
"criminal",
"justice",
"trial",
"prosecution"
),
minority_rights = c(
"education",
"diversity",
"multiculturalism",
"human rights",
"protection",
"social policy",
"tolerance",
"equity"
),
# **Jewish & Muslim Identity in Canada**
jewish_identity = c("jewish", "judaism", "diaspora", "synagogue", "rabbi", "Torah"),
muslim_identity = c("muslim", "islam", "mosque", "imam", "halal", "sharia", "hijab"),
# **Political Parties & Power Structures**
politics = c(
"liberal",
"conservative",
"ndp",
"parliament",
"government",
"leader",
"trudeau",
"harper",
"mulroney",
"party"
),
# **Historical Flashpoints (Wars, Major Events)**
historical_events = c(
"Six-Day War",
"Yom Kippur War",
"Oslo Accords",
"UN resolution",
"Camp David",
"Gaza War",
"Balfour Declaration"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
library(tidyverse)
library(quanteda)
library(keyATM)
library(readtext)
# Define output folder
# output_folder <- "speeches_text"
# dir.create(output_folder, showWarnings = FALSE)  # Create directory if it doesn't exist
# Save each speech as .txt
# for (i in 1:nrow(data)) {
#   file_name <- paste0(output_folder, "/speech_", i, ".txt")
#   writeLines(enc2utf8(data$speechtext[i]), file_name, useBytes = TRUE)  # Ensure UTF-8 encoding
# }
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)
# Ensure speechdate is in Date format
data$speechdate <- as.Date(data$speechdate)
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians", "Mr. Speaker", "can", "one", "also", "bill",
"members", "now") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts found after first rounds
unwanted_terms <- c(
# Metadata tags
"data-hocid", "data-originallang", "href", "title", "blockquote",
# Parliamentary jargon
"bills", "minister", "committee", "motion", "order",
# Generic government references
"government", "state", "house", "member", "prime",
# Broad, non-distinct terms
"politicians", "national", "report", "amend", "legislation"
# Context-dependent terms that appear in many discussions
)
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify data cleaning worked
topfeatures(dfm_clean, 20)
# Define keywords for Topic Modeling
keywords <- list(
antisemitism = c("antisemitism", "racism", "discrimination", "Jew-hatred", "anti-Jewish", "prejudice"),
israel_palestine = c("israel", "palestine", "zionism", "West Bank", "Gaza", "settlements", "two-state"),
holocaust = c("holocaust", "nazi", "genocide", "Shoah", "concentration camp", "Holocaust denial"),
jewish_identity = c("jewish", "judaism", "diaspora", "Jewish heritage", "Hebrew", "kashrut", "synagogue", "Yiddish")
)
# Convert DFM into keyATM format
keyATM_docs <- keyATM_read(texts = dfm_clean)
# Visualize keyword frequency
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz  # Plot frequency of each keyword
values_fig(key_viz)
# Save plot
# save_fig(key_viz, "figures/keyword_frequency.pdf", width = 6.5, height = 4)
library(quanteda)
library(keyATM)
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts (explicit cleaning)
unwanted_terms <- c("data-hocid", "data-originallang", "href", "title", "bills",
"minister", "committee", "politicians", "motion", "order",
"government", "state", "house", "member", "prime")
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix (DFM)
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify cleaning worked
topfeatures(dfm_clean, 20)
keyATM_docs <- keyATM_read(text)
keyATM_docs <- keyATM_read(texts = dfm_clean)
keywords <- list( Antisemitism = c("antisemitism", "racism", "discrimination"), Holocaust = c("holocaust", "genocide", "shoah"), IsraelPalestine = c("israel", "gaza", "zionism", "settlements"), JewishIdentity = c("jewish", "synagogue", "diaspora"), LegalActionFrame = c("law", "police", "court", "justice"), MinorityRightsFrame= c("education", "human", "rights", "diversity") )
set.seed(123)
model_out <- keyATM(docs = keyATM_docs, no_keyword_topics = 0, # no additional topics beyond our keyword topics keywords = keywords, model = "base", options = list(seed = 123, iterations = 1500) )
set.seed(123)
model_out <- keyATM(
docs              = keyATM_docs,
no_keyword_topics = 0,                # no additional topics beyond our keyword topics
keywords          = keywords,
model             = "base",
options           = list(seed = 123, iterations = 1500)
)
library(quanteda)
library(keyATM)
keywords <- list(
Antisemitism       = c("antisemitism", "racism", "discrimination"),
Holocaust          = c("holocaust", "genocide", "shoah"),
IsraelPalestine    = c("israel", "gaza", "zionism", "settlements"),
JewishIdentity     = c("jewish", "synagogue", "diaspora"),
LegalActionFrame   = c("law", "police", "court", "justice"),
MinorityRightsFrame= c("education", "human", "rights", "diversity")
)
saveRDS(model_out, "keyATM_jewish_discourse.rds")
keyword_plot <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
keyword_plot  # Display the plot
topicprop_plot <- plot_topicprop(model_out)
topicprop_plot
top_terms <- top_words(model_out, n = 10)  # top 10 words per topic
print(top_terms)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(topicmodels)
library(keyATM)
library(readtext)
library(tibble)
library(gt) # visually appealing tables
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Source the custom stopwords script
source("./stop_words/custom_stopwords.R")
# Tokenize and clean, using imported stopword list
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(pattern = custom_stops) %>%
tokens_select(min_nchar = 3)
# Convert to a Document-Feature Matrix and trim low-frequency terms
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 3, min_docfreq = 3) # Adjust thresholds as needed
# Remove empty docs
dfm_clean <- dfm_subset(dfm_clean, ntoken(dfm_clean) > 0)
# Check the top features for stopwords
# topfeatures(dfm_clean, 20)
keyATM_docs <- keyATM_read(texts = dfm_clean)
keywords <- list(
LegalAction = c("hate", "crime", "justice", "prosecution", "court"),
Israel = c(
"israel",
"israeli",
"zionism",
"zionist",
"IDF",
"war",
"military",
"airstrike",
"defense",
"operation",
"security"
),
Palestine = c(
"palestine",
"palestinian",
"gaza",
"refugee",
"displacement",
"blockade",
"humanitarian",
"aid",
"genocide"
)
)
# Convert DFM to DTM
dtm_clean <- convert(dfm_clean, to = "topicmodels")  # Now we get a cleaned DTM
# Standard LDA
lda_model <- LDA(
dtm_clean,
k = 8,
method = "VEM",
control = list(
seed = 123,
em   = list(iter.max = 1500)
)
)
# Save LDA model
saveRDS(lda_model, "./models/lda_model_hansard.rds")
# Extract top 10 terms per topic (topics as rows)
top_terms_mat <- terms(lda_model, 10)
# Convert matrix to a data frame and pivot to have topics as columns
top_terms_df <- as.data.frame(top_terms_mat, stringsAsFactors = FALSE)
# Convert row names (topics) into a column
top_terms_df <- rownames_to_column(top_terms_df, var = "Term")
# Reshape to have topics as columns
top_terms_df <- pivot_longer(top_terms_df, cols = -Term, names_to = "Topic", values_to = "Word") %>%
pivot_wider(names_from = Topic, values_from = Word)
# Create GT table
gt_table_lda <- top_terms_df %>%
gt() %>%
tab_header(
title = "Top 10 Terms per Unsupervised Topic",
subtitle = "LDA (k = 8) using VEM method"
) %>%
tab_style(
style = cell_text(align = "center"),
locations = cells_body(columns = everything())
)
# Display the table
gt_table_lda
set.seed(123)
model_out <- keyATM(
docs              = keyATM_docs,
no_keyword_topics = 5, # additional topics beyond our keyword topics;
# should help remove parliamentary noise.
keywords          = keywords,
model             = "base",
options           = list(seed = 123, iterations = 1500)
)
# Save keyATM model
saveRDS(model_out, "./models/keyATM_model_hansard.rds")
keyword_plot <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
keyword_plot  # Display the plot
topicprop_plot <- plot_topicprop(model_out)
topicprop_plot
# Check model fitting. If the model is working as expected, we would observe an increase trend for the log-likelihood and an decrease trend for the perplexity:
fig_modelfit <- plot_modelfit(model_out)
fig_modelfit
# Visualize alpha, the prior for the document-topic distribution, and the probability that each topic uses keyword topic-word distribution. These should stabilize over time:
plot_alpha(model_out)
# Probability of words drawn from keyword topic-word distribution:
plot_pi(model_out)
# Extract top 20 words per topic from keyATM model
top_terms <- top_words(model_out, n = 10)
# Convert list to data frame (each topic as a column)
top_terms_df <- as.data.frame(do.call(cbind, top_terms))
# Rename columns to match desired topic labels
colnames(top_terms_df) <- c("Legal Action", "Israel", "Palestine",
paste0("UT ", seq_len(ncol(top_terms_df) - 3)))
# Add term numbers as a row identifier
top_terms_df <- tibble(Term = paste0("Term ", seq_len(nrow(top_terms_df))), top_terms_df)
# Create gt table
gt_table_keyATM <- top_terms_df %>%
gt() %>%
tab_header(
title = "Top 10 Terms per keyATM Topic",
subtitle = "keyATM Model: Guided & Unsupervised Topics (UT)"
) %>%
tab_style(
style = cell_text(align = "center"),
locations = cells_body(columns = everything())
)
gt_table_keyATM
