"discrimination",
"racism",
"stereotypes",
"neo-nazi",
"white supremacy"
),
islamophobia = c(
"islamophobia",
"anti-Muslim",
"Muslim-hatred",
"prejudice",
"hate",
"discrimination",
"racism",
"stereotypes",
"white supremacy"
),
# **Israel vs. Palestine (Geopolitics)**
israel = c(
"israel",
"zionism",
"settlements",
"two-state",
"IDF",
"Netanyahu",
"Jerusalem",
"West Bank",
"Israeli government"
),
palestine = c(
"palestine",
"West Bank",
"Gaza",
"Hamas",
"PLO",
"Palestinian Authority",
"Intifada",
"resistance",
"occupation"
),
# **Holocaust & Genocide References**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"concentration camp",
"Holocaust denial",
"Auschwitz",
"Final Solution"
),
# **Jewish & Muslim Identity (Cultural & Religious Aspects)**
jewish_identity = c(
"jewish",
"judaism",
"diaspora",
"Jewish heritage",
"Hebrew",
"kashrut",
"synagogue",
"Yiddish",
"rabbi",
"Torah"
),
muslim_identity = c(
"muslim",
"islam",
"imam",
"mosque",
"Quran",
"halal",
"sharia",
"hijab",
"ummah",
"sunni",
"shia"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# View top words per topic
top_words(out)
# Convert DFM into keyATM format
keyATM_docs <- keyATM_read(texts = dfm_clean)
# Visualize keyword frequency
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz  # Plot frequency of each keyword
values_fig(key_viz)
# Save plot
# save_fig(key_viz, "figures/keyword_frequency.pdf", width = 6.5, height = 4)
# Keywords based on LDA output
keywords <- list(
# **Antisemitism & Islamophobia**
antisemitism = c(
"antisemitism",
"Jew-hatred",
"anti-Jewish",
"neo-nazi",
"white supremacy"
),
islamophobia = c(
"islamophobia",
"anti-Muslim",
"Muslim-hatred",
"Islamophobic",
"religious discrimination"
),
# **Israel vs. Palestine (Geopolitics)**
israel = c(
"israel",
"zionism",
"settlements",
"IDF",
"Netanyahu",
"Jerusalem"
),
palestine = c("palestine", "Gaza", "Hamas", "PLO", "Intifada", "occupation"),
# **Holocaust & Genocide**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"Auschwitz",
"Holocaust denial"
),
# **Jewish & Muslim Identity**
jewish_identity = c("jewish", "judaism", "diaspora", "synagogue"),
muslim_identity = c(
"muslim",
"islam",
"mosque",
"Quran",
"halal",
"sharia",
"Islamic practices"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# View top words per topic
top_words(out)
library(tidyverse)
library(quanteda)
library(keyATM)
library(readtext)
# Define output folder
# output_folder <- "speeches_text"
# dir.create(output_folder, showWarnings = FALSE)  # Create directory if it doesn't exist
# Save each speech as .txt
# for (i in 1:nrow(data)) {
#   file_name <- paste0(output_folder, "/speech_", i, ".txt")
#   writeLines(enc2utf8(data$speechtext[i]), file_name, useBytes = TRUE)  # Ensure UTF-8 encoding
# }
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians", "Mr. Speaker", "can", "one", "also", "bill",
"members", "now") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts (specifically identified in the texts)
unwanted_terms <- c(
# Metadata tags
"data-hocid", "data-originallang", "href", "title", "blockquote",
# Parliamentary jargon
"bills", "minister", "committee", "motion", "order",
# Generic government references
"government", "state", "house", "member", "prime",
# Broad, non-distinct terms
"politicians", "national", "report", "amend", "legislation"
# Context-dependent terms that appear in many discussions
)
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify data cleaning worked
topfeatures(dfm_clean, 20)
set.seed(225)
docs_withSplit <- keyATM_read(texts = dfm_clean, split = 0.3)  # Use 30% for LDA
out <- weightedLDA(
docs              = docs_withSplit$W_split,  # 30% of the corpus
number_of_topics  = 10,  # Define number of topics (k)
model             = "base",
options           = list(seed = 250)
)
# Extract top words from the model (to inform keyword selection)
top_words(out)
# Keywords based on LDA output
keywords <- list(
# **Antisemitism & Islamophobia**
antisemitism = c(
"antisemitism",
"Jew-hatred",
"anti-Jewish",
"neo-nazi",
"white supremacy"
),
islamophobia = c(
"islamophobia",
"anti-Muslim",
"Muslim-hatred",
"Islamophobic",
"religious discrimination"
),
# **Israel vs. Palestine (Geopolitics)**
israel = c(
"israel",
"zionism",
"settlements",
"IDF",
"Netanyahu",
"Jerusalem"
),
palestine = c("palestine", "Gaza", "Hamas", "PLO", "Intifada", "occupation"),
# **Holocaust & Genocide**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"Auschwitz",
"Holocaust denial"
),
# **Jewish & Muslim Identity**
jewish_identity = c("jewish", "judaism", "diaspora", "synagogue"),
muslim_identity = c(
"muslim",
"islam",
"mosque",
"Quran",
"halal",
"sharia",
"Islamic practices"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# View top words per topic
top_words(out)
# Visualize top words per topic
topic_df <- tidy(lda_model, matrix = "beta")
keywords <- list(
# **Antisemitism & Its Framing**
antisemitism = c(
"antisemitism",
"Jew-hatred",
"anti-Jewish",
"prejudice",
"hate",
"discrimination",
"racism",
"stereotypes",
"neo-nazi",
"white supremacy",
"legal",
"hate crime",
"speech laws"
),
# **Zionism & Israel-Palestine Conflict**
israel = c(
"israel",
"zionism",
"settlements",
"IDF",
"Netanyahu",
"Jerusalem",
"Jewish state",
"West Bank"
),
palestine = c(
"palestine",
"Gaza",
"Hamas",
"PLO",
"Intifada",
"occupation",
"Palestinian Authority",
"resistance",
"apartheid"
),
# **The Holocaust & Historical Memory**
holocaust = c(
"holocaust",
"nazi",
"genocide",
"Shoah",
"Auschwitz",
"Holocaust denial",
"Final Solution",
"remembrance"
),
# **Framing of Jewish Issues (Law vs. Minority Rights)**
legal_action = c(
"law",
"court",
"police",
"criminal",
"justice",
"trial",
"prosecution"
),
minority_rights = c(
"education",
"diversity",
"multiculturalism",
"human rights",
"protection",
"social policy",
"tolerance",
"equity"
),
# **Jewish & Muslim Identity in Canada**
jewish_identity = c("jewish", "judaism", "diaspora", "synagogue", "rabbi", "Torah"),
muslim_identity = c("muslim", "islam", "mosque", "imam", "halal", "sharia", "hijab"),
# **Political Parties & Power Structures**
politics = c(
"liberal",
"conservative",
"ndp",
"parliament",
"government",
"leader",
"trudeau",
"harper",
"mulroney",
"party"
),
# **Historical Flashpoints (Wars, Major Events)**
historical_events = c(
"Six-Day War",
"Yom Kippur War",
"Oslo Accords",
"UN resolution",
"Camp David",
"Gaza War",
"Balfour Declaration"
)
)
# Run keyATM model
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # Topics without predefined keywords
keywords          = keywords,        # Use identified keywords
model             = "base",
options           = list(seed = 250)
)
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
library(tidyverse)
library(quanteda)
library(keyATM)
library(readtext)
# Define output folder
# output_folder <- "speeches_text"
# dir.create(output_folder, showWarnings = FALSE)  # Create directory if it doesn't exist
# Save each speech as .txt
# for (i in 1:nrow(data)) {
#   file_name <- paste0(output_folder, "/speech_", i, ".txt")
#   writeLines(enc2utf8(data$speechtext[i]), file_name, useBytes = TRUE)  # Ensure UTF-8 encoding
# }
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)
# Ensure speechdate is in Date format
data$speechdate <- as.Date(data$speechdate)
# Read text files (if applicable)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians", "Mr. Speaker", "can", "one", "also", "bill",
"members", "now") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts found after first rounds
unwanted_terms <- c(
# Metadata tags
"data-hocid", "data-originallang", "href", "title", "blockquote",
# Parliamentary jargon
"bills", "minister", "committee", "motion", "order",
# Generic government references
"government", "state", "house", "member", "prime",
# Broad, non-distinct terms
"politicians", "national", "report", "amend", "legislation"
# Context-dependent terms that appear in many discussions
)
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify data cleaning worked
topfeatures(dfm_clean, 20)
# Define keywords for Topic Modeling
keywords <- list(
antisemitism = c("antisemitism", "racism", "discrimination", "Jew-hatred", "anti-Jewish", "prejudice"),
israel_palestine = c("israel", "palestine", "zionism", "West Bank", "Gaza", "settlements", "two-state"),
holocaust = c("holocaust", "nazi", "genocide", "Shoah", "concentration camp", "Holocaust denial"),
jewish_identity = c("jewish", "judaism", "diaspora", "Jewish heritage", "Hebrew", "kashrut", "synagogue", "Yiddish")
)
# Convert DFM into keyATM format
keyATM_docs <- keyATM_read(texts = dfm_clean)
# Visualize keyword frequency
key_viz <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
key_viz  # Plot frequency of each keyword
values_fig(key_viz)
# Save plot
# save_fig(key_viz, "figures/keyword_frequency.pdf", width = 6.5, height = 4)
library(quanteda)
library(keyATM)
# Load Data (modify path as necessary)
data <- read.csv("./data/openparliament_keyATM.csv", stringsAsFactors = FALSE)
raw_docs <- readtext("speeches_text/*.txt", encoding = "UTF-8")
# Convert to corpus
key_corpus <- corpus(raw_docs, text_field = "text")
# Tokenization & Cleaning (quanteda best practices)
tokens_clean <- tokens(
key_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE
) %>%
tokens_tolower() %>%
tokens_remove(
c(stopwords("english"), "shall", "may", "upon", "without", "canada",
"canadian", "canadians") # Corpus-specific stopwords
) %>%
tokens_select(min_nchar = 3)
# Remove metadata artifacts (explicit cleaning)
unwanted_terms <- c("data-hocid", "data-originallang", "href", "title", "bills",
"minister", "committee", "politicians", "motion", "order",
"government", "state", "house", "member", "prime")
tokens_clean <- tokens_remove(tokens_clean, pattern = unwanted_terms)
# Convert to Document-Feature Matrix (DFM)
dfm_clean <- dfm(tokens_clean) %>%
dfm_trim(min_termfreq = 5, min_docfreq = 2)  # Trim low-frequency terms
# Check top features to verify cleaning worked
topfeatures(dfm_clean, 20)
keyATM_docs <- keyATM_read(text)
keyATM_docs <- keyATM_read(texts = dfm_clean)
keywords <- list( Antisemitism = c("antisemitism", "racism", "discrimination"), Holocaust = c("holocaust", "genocide", "shoah"), IsraelPalestine = c("israel", "gaza", "zionism", "settlements"), JewishIdentity = c("jewish", "synagogue", "diaspora"), LegalActionFrame = c("law", "police", "court", "justice"), MinorityRightsFrame= c("education", "human", "rights", "diversity") )
set.seed(123)
model_out <- keyATM(docs = keyATM_docs, no_keyword_topics = 0, # no additional topics beyond our keyword topics keywords = keywords, model = "base", options = list(seed = 123, iterations = 1500) )
set.seed(123)
model_out <- keyATM(
docs              = keyATM_docs,
no_keyword_topics = 0,                # no additional topics beyond our keyword topics
keywords          = keywords,
model             = "base",
options           = list(seed = 123, iterations = 1500)
)
library(quanteda)
library(keyATM)
keywords <- list(
Antisemitism       = c("antisemitism", "racism", "discrimination"),
Holocaust          = c("holocaust", "genocide", "shoah"),
IsraelPalestine    = c("israel", "gaza", "zionism", "settlements"),
JewishIdentity     = c("jewish", "synagogue", "diaspora"),
LegalActionFrame   = c("law", "police", "court", "justice"),
MinorityRightsFrame= c("education", "human", "rights", "diversity")
)
saveRDS(model_out, "keyATM_jewish_discourse.rds")
keyword_plot <- visualize_keywords(docs = keyATM_docs, keywords = keywords)
keyword_plot  # Display the plot
topicprop_plot <- plot_topicprop(model_out)
topicprop_plot
top_terms <- top_words(model_out, n = 10)  # top 10 words per topic
print(top_terms)
